{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install langchain transformers sentence-transformers InstructEmbedding Accelerate pdfplumber tiktoken chromadb einops bitsandbytes"
      ],
      "metadata": {
        "id": "ZwEFWzet_iSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oH5Yfcj2Wjj"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import WebBaseLoader, PDFPlumberLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n",
        "import torch\n",
        "import re\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMB_OPENAI_ADA = \"text-embedding-ada-002\"\n",
        "EMB_INSTRUCTOR_XL = \"hkunlp/instructor-xl\"\n",
        "EMB_SBERT_MPNET_BASE = \"sentence-transformers/all-mpnet-base-v2\""
      ],
      "metadata": {
        "id": "JemQDcA26w4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LLM_OPENAI_GPT35 = \"gpt-3.5-turbo\"\n",
        "LLM_FLAN_T5_XXL = \"google/flan-t5-xxl\"\n",
        "LLM_FLAN_T5_XL = \"google/flan-t5-xl\"\n",
        "LLM_FASTCHAT_T5_XL = \"lmsys/fastchat-t5-3b-v1.0\"\n",
        "LLM_FLAN_T5_SMALL = \"google/flan-t5-small\"\n",
        "LLM_FLAN_T5_BASE = \"google/flan-t5-base\"\n",
        "LLM_FLAN_T5_LARGE = \"google/flan-t5-large\"\n",
        "LLM_FALCON_SMALL = \"tiiuae/falcon-7b-instruct\""
      ],
      "metadata": {
        "id": "uIwU-JmO6wn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PdfQA:\n",
        "    def __init__(self,config:dict = {}):\n",
        "        self.config = config\n",
        "        self.embedding = None\n",
        "        self.vectordb = None\n",
        "        self.llm = None\n",
        "        self.qa = None\n",
        "        self.retriever = None\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def create_instructor_xl(cls):\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        return HuggingFaceInstructEmbeddings(model_name=EMB_INSTRUCTOR_XL, model_kwargs={\"device\": device})\n",
        "\n",
        "    @classmethod\n",
        "    def create_sbert_mpnet(cls):\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, model_kwargs={\"device\": device})\n",
        "\n",
        "    @classmethod\n",
        "    def create_flan_t5_xxl(cls, load_in_8bit=False):\n",
        "        return pipeline(\n",
        "            task=\"text2text-generation\",\n",
        "            model=\"google/flan-t5-xxl\",\n",
        "            max_new_tokens=200,\n",
        "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.2}\n",
        "        )\n",
        "    @classmethod\n",
        "    def create_flan_t5_xl(cls, load_in_8bit=False):\n",
        "        return pipeline(\n",
        "            task=\"text2text-generation\",\n",
        "            model=\"google/flan-t5-xl\",\n",
        "            max_new_tokens=200,\n",
        "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.2}\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def create_flan_t5_small(cls, load_in_8bit=False):\n",
        "        model=\"google/flan-t5-small\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "        return pipeline(\n",
        "            task=\"text2text-generation\",\n",
        "            model=model,\n",
        "            tokenizer = tokenizer,\n",
        "            max_new_tokens=100,\n",
        "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.2}\n",
        "        )\n",
        "    @classmethod\n",
        "    def create_flan_t5_base(cls, load_in_8bit=False):\n",
        "        model=\"google/flan-t5-base\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "        return pipeline(\n",
        "            task=\"text2text-generation\",\n",
        "            model=model,\n",
        "            tokenizer = tokenizer,\n",
        "            max_new_tokens=100,\n",
        "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.2}\n",
        "        )\n",
        "    @classmethod\n",
        "    def create_flan_t5_large(cls, load_in_8bit=False):\n",
        "        model=\"google/flan-t5-large\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "        return pipeline(\n",
        "            task=\"text2text-generation\",\n",
        "            model=model,\n",
        "            tokenizer = tokenizer,\n",
        "            max_new_tokens=10000,\n",
        "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 4096, \"temperature\": 0.2}\n",
        "        )\n",
        "    @classmethod\n",
        "    def create_fastchat_t5_xl(cls, load_in_8bit=False):\n",
        "        return pipeline(\n",
        "            task=\"text2text-generation\",\n",
        "            model = \"lmsys/fastchat-t5-3b-v1.0\",\n",
        "            max_new_tokens=100,\n",
        "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.2}\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def create_falcon_instruct_small(cls, load_in_8bit=False):\n",
        "        model = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "        hf_pipeline = pipeline(\n",
        "                task=\"text-generation\",\n",
        "                model = model,\n",
        "                tokenizer = tokenizer,\n",
        "                trust_remote_code = True,\n",
        "                max_new_tokens=300,\n",
        "                model_kwargs={\n",
        "                    \"device_map\": \"auto\",\n",
        "                    \"load_in_8bit\": load_in_8bit,\n",
        "                    \"max_length\": 1024,\n",
        "                    \"temperature\": 0.2,\n",
        "                    \"torch_dtype\":torch.bfloat16,\n",
        "                    }\n",
        "            )\n",
        "        return hf_pipeline\n",
        "\n",
        "    def init_embeddings(self) -> None:\n",
        "        if self.config[\"embedding\"] == EMB_OPENAI_ADA:\n",
        "            self.embedding = OpenAIEmbeddings()\n",
        "        elif self.config[\"embedding\"] == EMB_INSTRUCTOR_XL:\n",
        "            if self.embedding is None:\n",
        "                self.embedding = PdfQA.create_instructor_xl()\n",
        "        elif self.config[\"embedding\"] == EMB_SBERT_MPNET_BASE:\n",
        "            if self.embedding is None:\n",
        "                self.embedding = PdfQA.create_sbert_mpnet()\n",
        "        else:\n",
        "            self.embedding = None\n",
        "\n",
        "    def init_models(self) -> None:\n",
        "        \"\"\" Initialize LLM models based on config \"\"\"\n",
        "        load_in_8bit = self.config.get(\"load_in_8bit\",False)\n",
        "        if self.config[\"llm\"] == LLM_OPENAI_GPT35:\n",
        "            pass\n",
        "        elif self.config[\"llm\"] == LLM_FLAN_T5_SMALL:\n",
        "            if self.llm is None:\n",
        "                self.llm = PdfQA.create_flan_t5_small(load_in_8bit=load_in_8bit)\n",
        "        elif self.config[\"llm\"] == LLM_FLAN_T5_BASE:\n",
        "            if self.llm is None:\n",
        "                self.llm = PdfQA.create_flan_t5_base(load_in_8bit=load_in_8bit)\n",
        "        elif self.config[\"llm\"] == LLM_FLAN_T5_LARGE:\n",
        "            if self.llm is None:\n",
        "                self.llm = PdfQA.create_flan_t5_large(load_in_8bit=load_in_8bit)\n",
        "        elif self.config[\"llm\"] == LLM_FLAN_T5_XL:\n",
        "            if self.llm is None:\n",
        "                self.llm = PdfQA.create_flan_t5_xl(load_in_8bit=load_in_8bit)\n",
        "        elif self.config[\"llm\"] == LLM_FLAN_T5_XXL:\n",
        "            if self.llm is None:\n",
        "                self.llm = PdfQA.create_flan_t5_xxl(load_in_8bit=load_in_8bit)\n",
        "        elif self.config[\"llm\"] == LLM_FASTCHAT_T5_XL:\n",
        "            if self.llm is None:\n",
        "                self.llm = PdfQA.create_fastchat_t5_xl(load_in_8bit=load_in_8bit)\n",
        "        elif self.config[\"llm\"] == LLM_FALCON_SMALL:\n",
        "            if self.llm is None:\n",
        "                self.llm = PdfQA.create_falcon_instruct_small(load_in_8bit=load_in_8bit)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Invalid config\")\n",
        "    def vector_db_pdf(self) -> None:\n",
        "        \"\"\"\n",
        "        creates vector db for the embeddings and persists them or loads a vector db from the persist directory\n",
        "        \"\"\"\n",
        "        pdf_path = self.config.get(\"pdf_path\",None)\n",
        "        persist_directory = self.config.get(\"persist_directory\",None)\n",
        "        if persist_directory and os.path.exists(persist_directory):\n",
        "            self.vectordb = Chroma(persist_directory=persist_directory, embedding_function=self.embedding)\n",
        "        elif pdf_path and os.path.exists(pdf_path):\n",
        "            loader = PDFPlumberLoader(pdf_path)\n",
        "            documents = loader.load()\n",
        "            text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "            texts = text_splitter.split_documents(documents)\n",
        "            text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=10)\n",
        "            texts = text_splitter.split_documents(texts)\n",
        "\n",
        "            self.vectordb = Chroma.from_documents(documents=texts, embedding=self.embedding, persist_directory=persist_directory)\n",
        "        else:\n",
        "            raise ValueError(\"NO PDF found\")\n",
        "\n",
        "    def retreival_qa_chain(self):\n",
        "        \"\"\"\n",
        "        Creates retrieval qa chain using vectordb as retrivar and LLM to complete the prompt\n",
        "        \"\"\"\n",
        "        self.retriever = self.vectordb.as_retriever(search_kwargs={\"k\":3})\n",
        "\n",
        "        if self.config[\"llm\"] == LLM_OPENAI_GPT35:\n",
        "\n",
        "          self.qa = RetrievalQA.from_chain_type(llm=OpenAI(model_name=LLM_OPENAI_GPT35, temperature=0.), chain_type=\"stuff\",\\\n",
        "                                      retriever=self.vectordb.as_retriever(search_kwargs={\"k\":3}))\n",
        "        else:\n",
        "            hf_llm = HuggingFacePipeline(pipeline=self.llm,model_id=self.config[\"llm\"])\n",
        "\n",
        "            self.qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"stuff\",retriever=self.retriever)\n",
        "            if self.config[\"llm\"] == LLM_FLAN_T5_SMALL or self.config[\"llm\"] == LLM_FLAN_T5_BASE or self.config[\"llm\"] == LLM_FLAN_T5_LARGE:\n",
        "                question_t5_template = \"\"\"\n",
        "                context: {context}\n",
        "                question: {question}\n",
        "                answer:\n",
        "                \"\"\"\n",
        "                QUESTION_T5_PROMPT = PromptTemplate(\n",
        "                    template=question_t5_template, input_variables=[\"context\", \"question\"]\n",
        "                )\n",
        "                self.qa.combine_documents_chain.llm_chain.prompt = QUESTION_T5_PROMPT\n",
        "            self.qa.combine_documents_chain.verbose = True\n",
        "            self.qa.return_source_documents = True\n",
        "    def answer_query(self,question:str) ->str:\n",
        "        \"\"\"\n",
        "        Answer the question\n",
        "        \"\"\"\n",
        "\n",
        "        answer_dict = self.qa({\"query\":question,})\n",
        "        print(answer_dict)\n",
        "        answer = answer_dict[\"result\"]\n",
        "        if self.config[\"llm\"] == LLM_FASTCHAT_T5_XL:\n",
        "            answer = self._clean_fastchat_t5_output(answer)\n",
        "        return answer\n",
        "    def _clean_fastchat_t5_output(self, answer: str) -> str:\n",
        "        answer = re.sub(r\"<pad>\\s+\", \"\", answer)\n",
        "        answer = re.sub(r\"  \", \" \", answer)\n",
        "        answer = re.sub(r\"\\n$\", \"\", answer)\n",
        "        return answer"
      ],
      "metadata": {
        "id": "VcnQi2525e8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"persist_directory\":None,\n",
        "          \"load_in_4bit\":True,\n",
        "          \"embedding\" : EMB_SBERT_MPNET_BASE,\n",
        "          \"llm\":FLAN_T5_BASE,\n",
        "          \"pdf_path\":\"path to pdf\"\n",
        "          }\n",
        "\n",
        "\n",
        "pdfqa = PdfQA(config=config)\n",
        "pdfqa.init_embeddings()\n",
        "pdfqa.init_models()\n",
        "\n",
        "\n",
        "pdfqa.vector_db_pdf()\n",
        "\n",
        "\n",
        "pdfqa.retreival_qa_chain()\n",
        "\n",
        "\n",
        "question = \"\"\n",
        "pdfqa.answer_query(question)\n",
        "\n"
      ],
      "metadata": {
        "id": "RNBXjBSG6fdl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}